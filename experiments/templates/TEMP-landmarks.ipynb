{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deltascope as ds\n",
    "import deltascope.alignment as ut\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize, Normalizer\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "The user needs to specify the directories containing the data of interest (after ilastik processing). Each sample type should have a key which corresponds to the directory path. Additionally, each object should have a list that includes the channels of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# -------- User input ------------\n",
    "# --------------------------------\n",
    "\n",
    "data = {\n",
    "    # Specify first sample type name. ex: 'wt'\n",
    "    'stype1': {\n",
    "        # Specify path to data directory\n",
    "        'path': 'path_to_data_prob_directory_for_first_sample_type',\n",
    "        # Specify which channels are in the directory and are of interest\n",
    "        'channels': ['ch1','ch2'] # ex: ['AT','ZRF']\n",
    "    },\n",
    "    # Specify second sample type name. ex: 'yot'\n",
    "    'stype2': {\n",
    "        # Specify path to data directory\n",
    "        'path': 'path_to_data_prob_directory_for_second_sample_type',\n",
    "        # Specify which channels are in the directory and are of interest\n",
    "        'channels': ['ch1','ch2'] # ex: ['AT','ZRF']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a list of pairs of stypes and channels for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs = []\n",
    "for s in data.keys():\n",
    "    for c in data[s]['channels']:\n",
    "        data_pairs.append((s,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read in all datafiles specified by the `data` dictionary above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = {}\n",
    "for s in data.keys():\n",
    "    D[s] = {}\n",
    "    for c in data[s]['channels']:\n",
    "        D[s][c] = ds.read_psi_to_dict(data[s]['path'],c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate optimum landmark parameters\n",
    "Choose a single sample type and channel which will be used as the control to calculate the appropriate size of landmark bins. Typically we use wildtype samples and the AT (structural) channel. We will run a parameter sweep of bin sizes using ds.anumSelect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# -------- User input ------------\n",
    "# --------------------------------\n",
    "\n",
    "# Specify sample type to use as control. ex: 'wt'\n",
    "s_ctrl = 'control_sample_type'\n",
    "# Specify channel to use as control. ex: 'AT'\n",
    "c_ctrl = 'control_channel'\n",
    "\n",
    "# Specify size of theta bins in radians\n",
    "theta_step = np.pi/4\n",
    "\n",
    "# Specify minimum a value when sweeping various alpha bin sizes\n",
    "amn = 2\n",
    "# Specify maximum a value\n",
    "amx = 50\n",
    "# Specify the step size in a value\n",
    "astep = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optr = ds.anumSelect(D[s_ctrl][c_ctrl])\n",
    "optr.param_sweep(theta_step, amn=amn, amx=amx, astep=astep,\n",
    "                 rnull=np.nan, DT='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot result of the parameter sweep to get a sense of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# -------- User input ------------\n",
    "# --------------------------------\n",
    "\n",
    "# Select degrees of freedom for curve fitting\n",
    "dof = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(amn,amx,astep)\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Fit a curve to bin variance values\n",
    "optrMbvarray = np.array(optr.Mbv).reshape(1, len(optr.Mbv))\n",
    "pbv = np.polyfit(x, normalize(optrMbvarray)[0], dof)\n",
    "fbv = np.poly1d(pbv)\n",
    "ax.plot(x, fbv(x), c='b', label='Bin Variance')\n",
    "\n",
    "# Fit curve to sample variance values\n",
    "optrMsvarray = np.array(optr.Msv).reshape(1, len(optr.Msv))\n",
    "psv = np.polyfit(x, normalize(optrMsvarray)[0], dof)\n",
    "fsv = np.poly1d(psv)\n",
    "ax.plot(x, fsv(x), c='g', label='Sample Variance')\n",
    "\n",
    "# Plot sum of sample and bin variances\n",
    "ax.plot(x, fsv(x)+fbv(x), c='c', label='Total Variance')\n",
    "\n",
    "ax.legend()\n",
    "tstamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "location = \"directory_to_save_figure_to\"  #MODIFY PATH HERE\n",
    "fig.savefig(location+tstamp+'_yotlandmarkoptimization-{}-{}.png'.format(c_ctrl,s_ctrl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the optimal bin size with the help of the user specifying a best guess for the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# -------- User input ------------\n",
    "# --------------------------------\n",
    "\n",
    "# Guess the approximate value on the horizontal axis where total variance is minimized\n",
    "guess = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = minimize(fbv+fsv, guess)\n",
    "ax.axvline(opt.x, c='r', label='Optimum: '+str(np.round(opt.x[0], 2)))\n",
    "print(opt.x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate landmark bins\n",
    "Based on the analysis above, we can select the optimal value of alpha bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# -------- User input ------------\n",
    "# --------------------------------\n",
    "\n",
    "# Pick an integer value for bin number based on results above\n",
    "anum = 25\n",
    "\n",
    "# Specify the percentile(s) which will be used to calculate landmarks\n",
    "percbins = [50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate landmark bins based on user input parameters and the previously specified control sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ds.landmarks(percbins=percbins, rnull=np.nan)\n",
    "lm.calc_bins(D[s_ctrl][c_ctrl], anum, theta_step)\n",
    "\n",
    "print('Alpha bins')\n",
    "print(lm.acbins)\n",
    "print('Theta bins')\n",
    "print(lm.tbins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmdf = pd.DataFrame()\n",
    "\n",
    "# Loop through each pair of stype and channels\n",
    "for s,c in tqdm.tqdm(data_pairs):\n",
    "    print(s,c)\n",
    "    # Calculate landmarks for each sample with this data pair\n",
    "    for k,df in tqdm.tqdm(D[s][c].items()):\n",
    "        lmdf = lm.calc_perc(df, k, '-'.join([s,c]), lmdf)\n",
    "        \n",
    "# Set timestamp for saving data\n",
    "tstamp = time.strftime(\"%m-%d-%H-%M\",time.localtime())\n",
    "        \n",
    "# Save completed landmarks to a csv file\n",
    "lmdf.to_csv(tstamp+'_landmarks.csv')\n",
    "print('Landmarks saved to csv')\n",
    "\n",
    "# Save landmark bins to json file\n",
    "bins = {\n",
    "    'acbins':list(lm.acbins),\n",
    "    'tbins':list(lm.tbins)\n",
    "}\n",
    "with open(tstamp+'_landmarks_bins.json', 'w') as outfile:\n",
    "    json.dump(bins, outfile)\n",
    "print('Bins saved to json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
